{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "*Team: nada401*\n",
    "\n",
    "In this Notebook we present the initial results of our project.\n",
    "\n",
    "Structure:\n",
    "- Data cleaning and exploration\n",
    "    - Language tagging\n",
    "    - Data cleaning\n",
    "- Expert-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility\n",
    "\n",
    "To reproduce the results of this notebook first download the datasets from [here](https://drive.google.com/drive/folders/1Wz6D2FM25ydFw_-41I9uTwG9uNsN4TCF) and unzip them in `./data/`.\n",
    "\n",
    "Only the datasets\n",
    "- `RateBeer`\n",
    "- `BeerAdvocate`\n",
    "are used for this part of the project.\n",
    "\n",
    "### Notation\n",
    "\n",
    "To distinguish the two datasets, a common naming scheme has been used, in particular:\n",
    "- `*_RB` contains data from RateBeer\n",
    "- `*_BA` contains data from BeerAdvocate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and exploration\n",
    "\n",
    "The first step into developing this project is cleaning and exploring the data.\n",
    "\n",
    "### Language tagging\n",
    "\n",
    "A first manual analysis of the datasets revealed that some reviews have been written in different languages, so we first wanted to understand the share of each language to direct our successive steps.\n",
    "\n",
    "To achieve this, we tested different python packages that tag language, among which:\n",
    "- [fast_langdetect](https://github.com/LlmKira/fast-langdetect)\n",
    "- [langdetect](https://pypi.org/project/langdetect/)\n",
    "- [lingua](https://github.com/pemistahl/lingua-py)\n",
    "\n",
    "Finally opting for the first, being the fastest and having good precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.lang_tagger import lang_tagger\n",
    "\n",
    "'''\n",
    "Execute the language tagging process.\n",
    "This function calls the pipeline that reads the .txt.gz files and creates a .csv file\n",
    "with only the reviews and few other columns for indexing purposes.\n",
    "'''\n",
    "lang_tagger.tag_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_BA = pd.read_csv(\"./data/BeerAdvocate/reviews_tagged.csv\")\n",
    "d_RB = pd.read_csv(\"./data/RateBeer/reviews_tagged.csv\")\n",
    "\n",
    "eng_perc_BA = d_BA[\"lang_tag\"].value_counts()['en']/d_BA[\"lang_tag\"].count() * 100\n",
    "eng_perc_RB = d_RB[\"lang_tag\"].value_counts()['en']/d_RB[\"lang_tag\"].count() * 100\n",
    "\n",
    "print(f\"Percentage of English reviews in BeerAdvocate = {eng_perc_BA:.3f}%\")\n",
    "print(f\"Percentage of English reviews in RateBeer = {eng_perc_RB:.3f}%\")\n",
    "\n",
    "print(f\"\\nNumber of reviews for the first 5 most used languages in Ratebeer. Only the first 5 shown for visualization purposes\")\n",
    "print(f\"{d_RB['lang_tag'].value_counts()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of reviews are made in English, especially in BeerAdvocate. \n",
    "We therefore initially focus on solely English reviews, specifically the one in BeerAdvocate, but we plan later in the development of the project to work also on RateBeer and we to test our metrics on different languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory after showing the results\n",
    "del d_BA, d_RB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "The dataset contained various NaN values and duplicated rows, all of which must be properly handled to ensure a correct analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.data_cleaning import data_cleaning, load_file\n",
    "\n",
    "data_cleaning.clean_data('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: show some statistics from the cleaning\n",
    "e.g.:\n",
    "- difference in the size of dataframes before and after cleaning\n",
    "- duplicated values\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert metric\n",
    "\n",
    "The vast majority of the work done in Milestone 2 for the project has been finding a good \"Expert metric\" and checking that our assumptions are correct.\n",
    "\n",
    "We needed a \"Expert metric\" that was reliable enough to see how written reviews change over time. This score should highlight expertise and precision of a review.\n",
    "\n",
    "We tried different methods to get this metric:\n",
    "- language depth\n",
    "- embeddings\n",
    "- ad-hoc metric\n",
    "\n",
    "The first two didn't provide a significant score, while a \"ad-hoc metric\" proved to satisfy our needs. We therefore start by discussing this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad-hoc metric\n",
    "\n",
    "Due to the failure of previous general metrics, we tried creating a topic-specific metric that focuses solely on beer reviews.\n",
    "\n",
    "**CONTINUE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language depth\n",
    "\n",
    "The assumption for this method was that a reviewers with more expertise would utilize a more nuanced vocabulary in their reviews.\n",
    "\n",
    "To extract language depth of a review we used\n",
    "[LexicalRichness](https://github.com/LSYS/LexicalRichness),\n",
    "a python package that extracts some metrics highlighting language richness of a text.<br>\n",
    "This has been used to evaluate each written reviews.\n",
    "\n",
    "However, the anlysis that followed provided unsatisfactory results. Our main explanation is that language richness doesn't directly correlate to higher quality reviews, as the metrics extrapolated by the package are not tailored to reviews nor beer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
