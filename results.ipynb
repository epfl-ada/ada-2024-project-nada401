{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "*Team: nada401*\n",
    "\n",
    "In this Notebook we present the initial results of our project.\n",
    "\n",
    "Structure:\n",
    "- Data cleaning and exploration\n",
    "    - Language tagging\n",
    "    - Data cleaning\n",
    "    - Data exploration\n",
    "- Expert-metric\n",
    "    - Ad-hoc metric\n",
    "    - Language depth\n",
    "    - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility\n",
    "\n",
    "To reproduce the results of this notebook first download the datasets from [here](https://drive.google.com/drive/folders/1Wz6D2FM25ydFw_-41I9uTwG9uNsN4TCF) and unzip them in `./data/`.\n",
    "\n",
    "Only the datasets\n",
    "- `RateBeer`\n",
    "- `BeerAdvocate`\n",
    "\n",
    "are used for this part of the project.\n",
    "\n",
    "### Notation\n",
    "\n",
    "To distinguish the two datasets, a common naming scheme has been used, in particular:\n",
    "- `*_RB` contains data from RateBeer\n",
    "- `*_BA` contains data from BeerAdvocate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and exploration\n",
    "\n",
    "The first step into developing this project is cleaning and exploring the data.\n",
    "\n",
    "### Language tagging\n",
    "\n",
    "A first manual analysis of the datasets revealed that some reviews have been written in different languages, so we first wanted to understand the share of each language to direct our successive steps.\n",
    "\n",
    "To achieve this, we tested different python packages that tag language, among which:\n",
    "- [fast_langdetect](https://github.com/LlmKira/fast-langdetect)\n",
    "- [langdetect](https://pypi.org/project/langdetect/)\n",
    "- [lingua](https://github.com/pemistahl/lingua-py)\n",
    "\n",
    "Finally opting for the first, being the fastest and having good precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.lang_tagger import lang_tagger\n",
    "\n",
    "'''\n",
    "Execute the language tagging process.\n",
    "This function calls the pipeline that reads the .txt.gz files and creates a .csv file\n",
    "with only the reviews and few other columns for indexing purposes.\n",
    "'''\n",
    "lang_tagger.tag_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_BA = pd.read_csv(\"./data/BeerAdvocate/reviews_tagged.csv\")\n",
    "d_RB = pd.read_csv(\"./data/RateBeer/reviews_tagged.csv\")\n",
    "\n",
    "eng_perc_BA = d_BA[\"lang_tag\"].value_counts()['en']/d_BA[\"lang_tag\"].count() * 100\n",
    "eng_perc_RB = d_RB[\"lang_tag\"].value_counts()['en']/d_RB[\"lang_tag\"].count() * 100\n",
    "\n",
    "print(f\"Percentage of English reviews in BeerAdvocate = {eng_perc_BA:.3f}%\")\n",
    "print(f\"Percentage of English reviews in RateBeer = {eng_perc_RB:.3f}%\")\n",
    "\n",
    "print(f\"\\nNumber of reviews for the first 5 most used languages in RateBeer. Only the first 5 shown for visualization purposes\")\n",
    "print(f\"{d_RB['lang_tag'].value_counts()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of reviews are made in English, especially in BeerAdvocate. \n",
    "We therefore initially focus on solely English reviews, specifically the one in BeerAdvocate, but we plan later in the development of the project to work also on RateBeer and we to test our metrics on different languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory after showing the results\n",
    "del d_BA, d_RB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "The dataset contained various NaN values and duplicated rows, all of which must be properly handled to ensure a correct analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.data_cleaning import data_cleaning, load_file\n",
    "\n",
    "'''\n",
    "Load the datasets and clean them. In particular:\n",
    "- drop duplicates\n",
    "- treat NaNs\n",
    "- delete beers that don't have reviews\n",
    "- delete users that didn't review at least one beer\n",
    "- add language_tag column to the datasets\n",
    "'''\n",
    "data_cleaning.clean_data('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Issues with users.csv\n",
    "df_users_RB = pd.read_csv('./data/RateBeer/users.csv')\n",
    "df_users_RB_clean = pd.read_csv('./data/RateBeer/users_RB_clean.csv')\n",
    "\n",
    "print(f\"Are users_id in BeerAdvocate's user dataframe unique? {df_users_RB['user_id'].is_unique}\")\n",
    "print(f\"By removing users that never did written reviews we dropped {df_users_RB.shape[0] - df_users_RB_clean.shape[0]} rows\")\n",
    "print(f\"Rows before cleaning: {df_users_RB.shape[0]}\\nRows after cleaning:  {df_users_RB_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language tagging and formatting\n",
    "df_ratings_BA_clean = pd.read_csv('./data/BeerAdvocate/ratings_BA_clean.csv', nrows=5)\n",
    "df_ratings_BA_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory after showing the results\n",
    "del df_users_RB, df_users_RB_clean, df_ratings_BA_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vik's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert metric\n",
    "\n",
    "The vast majority of the work done in Milestone 2 for the project has been finding a good \"Expert metric\" and checking that our assumptions are correct.\n",
    "\n",
    "We needed a \"Expert metric\" that was reliable enough to see how written reviews change over time. This score should highlight expertise and precision of a review.\n",
    "\n",
    "We tried different methods to get this metric:\n",
    "- language depth\n",
    "- embeddings\n",
    "- ad-hoc metric\n",
    "\n",
    "The first two didn't provide a significant score, while a \"ad-hoc metric\" proved to satisfy our needs. We therefore start by discussing this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad-hoc metric\n",
    "\n",
    "Due to the failure of previous general metrics, we tried creating a topic-specific metric that focuses solely on beer reviews.<br>\n",
    "\n",
    "**CONTINUE WRITING THE EXPLANATION**\n",
    "\n",
    "For this part, we will only use BeerAdvocate's dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.expert_metric import expert_metric\n",
    "\n",
    "data_folder = './data'\n",
    "expert_metric.add_ex_score_BA(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/BeerAdvocate/reviews_with_exp_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.expert_metric import expert_analysis\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "data_folder = './data'\n",
    "\n",
    "rev_with_scores, beers, users = expert_analysis.get_expert_metric_dfs(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>beer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>flavor</th>\n",
       "      <th>aroma</th>\n",
       "      <th>mouthfeel</th>\n",
       "      <th>brewing</th>\n",
       "      <th>technical</th>\n",
       "      <th>appearance</th>\n",
       "      <th>judgment</th>\n",
       "      <th>off_flavors</th>\n",
       "      <th>miscellaneous</th>\n",
       "      <th>expertness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nmann08.184925</td>\n",
       "      <td>142544</td>\n",
       "      <td>2015-08-20</td>\n",
       "      <td>From a bottle, pours a piss yellow color with ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         user_id  beer_id       date  \\\n",
       "0           0  nmann08.184925   142544 2015-08-20   \n",
       "\n",
       "                                                text  flavor  aroma  \\\n",
       "0  From a bottle, pours a piss yellow color with ...       2      3   \n",
       "\n",
       "   mouthfeel  brewing  technical  appearance  judgment  off_flavors  \\\n",
       "0          2        1          0           2         0            1   \n",
       "\n",
       "   miscellaneous  expertness_score  \n",
       "0              1                12  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_with_scores.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['flavor', 'aroma', 'mouthfeel', 'brewing', 'technical', 'appearance',\n",
      "       'judgment', 'off_flavors', 'miscellaneous', 'expertness_score',\n",
      "       'user_id'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.25250688377961705, pvalue=0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_to_keep = ['flavor', 'aroma', 'mouthfeel', 'brewing', 'technical', 'appearance', 'judgment','off_flavors', 'miscellaneous', 'expertness_score']\n",
    "user_ba = rev_with_scores.groupby('user_id').agg(\n",
    "    {col: 'mean' for col in col_to_keep} | {'user_id': 'count'}\n",
    ")\n",
    "\n",
    "print(user_ba.columns)\n",
    "\n",
    "user_ba = user_ba.rename(columns={'user_id': 'nbr_rev'})\n",
    "#! TODO: Finish doing\n",
    "pearsonr(user_ba['expertness_score'], user_ba['nbr_rev'])\n",
    "user_ba_less_200 = user_ba[user_ba['nbr_rev']<200]\n",
    "pearsonr(user_ba_less_200['expertness_score'], user_ba_less_200['nbr_rev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language depth\n",
    "\n",
    "The assumption for this method was that a reviewers with more expertise would utilize a more nuanced vocabulary in their reviews.\n",
    "\n",
    "To extract language depth of a review we used\n",
    "[LexicalRichness](https://github.com/LSYS/LexicalRichness),\n",
    "a python package that extracts some metrics highlighting language richness of a text.<br>\n",
    "This has been used to evaluate each written reviews.\n",
    "\n",
    "However, the anlysis that followed provided unsatisfactory results. Our main explanation is that language richness doesn't directly correlate to higher quality reviews, as the metrics extrapolated by the package are not tailored to reviews nor beer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "We also tried with finding the embeddings of the reviews and trying to see if such a method could be used as an expertise metric.<br>\n",
    "To find the embeddings we used:\n",
    "- [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) \n",
    "\n",
    "This model maps paragraphs to a 768 dimensional dense vector space, that we then visualize by projecting it in a 2D space with t-SNE and PCA.\n",
    "\n",
    "While some interesting patterns have been observed, it is clear that embeddings can't be used as a metric score.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
